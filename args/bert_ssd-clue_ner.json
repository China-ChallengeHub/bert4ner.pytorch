{
    "version": "v0",
    "device": "cuda:0",
    "n_gpu": 1,
    "task_name": "ner",
    "dataset_name": "clue_ner",
    "data_dir": "CLUENER2000/",
    "train_file": "train.json",
    "dev_file": "dev.json",
    "test_file": "test.json",
    "model_type": "bert_ssd",
    "model_name_or_path": "/media/louishsu/disk/Garage/weights/transformers/chinese-roberta-wwm-ext/",
    "output_dir": "outputs/",
    "anchor_size": [1, 2, 3, 4, 5, 6, 8, 9, 11, 13, 15],
    "iou_thresh_pos": 0.5,
    "iou_thresh_neg": 0.25,
    "weight_conf": 1.0,
    "weight_cls": 1.0,
    "weight_reg": 1.0,
    "conf_thresh": 0.7,
    "nms_thresh": 0.25,
    "scheme": "IOB2",
    "loss_type": "ce",
    "config_name": "",
    "tokenizer_name": "",
    "cache_dir": "cache/",
    "train_max_seq_length": 256,
    "eval_max_seq_length": 512,
    "do_train": true,
    "do_eval": true,
    "do_predict": true,
    "evaluate_during_training": true,
    "evaluate_each_epoch": true,
    "do_lower_case": true,
    "do_fgm": false,
    "fgm_epsilon": 1.0,
    "fgm_name": "word_embeddings",
    "per_gpu_train_batch_size": 24,
    "per_gpu_eval_batch_size": 12,
    "gradient_accumulation_steps": 1,
    "learning_rate": 2e-05,
    "other_learning_rate": 1e-3,
    "weight_decay": 0.01,
    "adam_epsilon": 1e-08,
    "max_grad_norm": 1.0,
    "num_train_epochs": 4.0,
    "max_steps": -1,
    "warmup_proportion": 0.1,
    "logging_steps": 50,
    "save_steps": 50,
    "save_best_checkpoints": true,
    "eval_all_checkpoints": false,
    "predict_checkpoints": 0,
    "no_cuda": false,
    "overwrite_output_dir": true,
    "seed": 42,
    "fp16": false,
    "fp16_opt_level": "O1",
    "local_rank": -1
}